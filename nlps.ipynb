{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "punkt is a pre-trained tokenizer model in NLTK (Natural Language Toolkit) used for unsupervised tokenization of text. It helps split text into words and sentences without requiring explicit rules for every language.\n",
    "punkt_tab is an internal resource used by punkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sandeep.C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Sandeep.C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['NLP', '--', 'dff', 'is', '>', ',', 'amazing', '!', 'I', 'love', 'learning', 'it', '.']\n",
      "Sentence Tokens: ['NLP -- dff is >, amazing!', 'I love learning it.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = \"NLP -- dff is >, amazing! I love learning it.\"\n",
    "word_tokens = word_tokenize(text)\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.', 'there.', 'How are.', 'you?', \"I'm learning NLP.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello. there. How are. you? I'm learning NLP.\"\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The event (held yesterday) was amazing!']\n"
     ]
    }
   ],
   "source": [
    "text = \"The event (held yesterday) was amazing!\"\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Brown, Ph.D., is an expert.', \"He wrote a paper titled 'AI & NLP: A Revolution.'\", \"It's quite famous.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"Dr. Brown, Ph.D., is an expert. He wrote a paper titled 'AI & NLP: A Revolution.' It's quite famous.\"\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stopwords Removal\n",
    "\n",
    "Stopwords are common words that don’t carry much meaning and are often removed from text before further processing.\n",
    "Examples: \"is\", \"the\", \"and\", \"in\", \"on\", \"at\".\n",
    "Reduces the size of the text data.\n",
    "Improves efficiency by focusing on meaningful words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sandeep.C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['This', 'is', 'an', 'amazing', 'NLP', 'tutorial', '.']\n",
      "Without Stopwords: ['amazing', 'NLP', 'tutorial', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"This is an amazing NLP tutorial.\"\n",
    "words = word_tokenize(text)\n",
    "# print(words)\n",
    "\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "print(\"Original:\", words)\n",
    "print(\"Without Stopwords:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced: Custom Stopword List\n",
    "We can define our own stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'amazing', 'tutorial.i', 'love', 'this', 'topic', ',', 'i', 'not', 'something']\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = {\"nlp\", \"tutorial\",\"do\",\"like\"}\n",
    "text = \"This is an amazing NLP tutorial.i love this topic, i do not like something\"\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [word for word in words if word.lower() not in custom_stopwords]\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['This', 'is', 'not', 'just', 'an', 'NLP', 'tutorial', ',', 'but', 'a', 'great', 'learning', 'experience', '.']\n",
      "Filtered Words: ['not', 'NLP', 'tutorial', ',', 'but', 'great', 'learning', 'experience', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = \"This is not just an NLP tutorial, but a great learning experience.\"\n",
    "\n",
    "# Tokenize words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Get default English stopwords\n",
    "default_stopwords = set(stopwords.words(\"english\"))\n",
    "# print(len(default_stopwords))\n",
    "\n",
    "# Define words to KEEP (exclude them from removal)\n",
    "words_to_keep = {\"not\", \"but\"}  \n",
    "\n",
    "# # Create a modified stopword set by removing the words_to_keep\n",
    "modified_stopwords = default_stopwords - words_to_keep  # Subtracting words_to_keep\n",
    "\n",
    "# # Filter words\n",
    "filtered_words = [word for word in words if word.lower() not in modified_stopwords]\n",
    "\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Filtered Words:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Stemming(Reducing Words to Their Root Form) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, \"running\" → \"run\", \"flies\" → \"fli\" (but this can be inaccurate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'run', 'dog', 'are', 'studi', 'harder', 'than', 'other', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "text = \"The running dogs are studying harder than others.\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "# words = [\"running\", \"flies\", \"easily\", \"loving\"]\n",
    "\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization (Context-Aware Root Word Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is smarter because it converts words to their dictionary form (lemma) using linguistic rules.\n",
    "For example, \"running\" → \"run\", \"flies\" → \"fly\" (correct!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sandeep.C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'running', 'dog', 'are', 'studying', 'harder', 'than', 'others', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"The running dogs are studying harder than others.\"\n",
    "\n",
    "# Tokenize words\n",
    "words = word_tokenize(text)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "# lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔹 4. Part-of-Speech (POS) Tagging\n",
    "POS tagging assigns grammatical labels (noun, verb, adjective, etc.) to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sandeep.C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Sandeep.C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'is', 'running', 'fast', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = word_tokenize(\"John is running fast.\")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('is', 'VBZ'), ('running', 'VBG'), ('fast', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pos_tags = pos_tag(sentence)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization with POS (Better Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words (with POS): ['The', 'run', 'dog', 'be', 'study', 'harder', 'than', 'others', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Apply lemmatization with POS tagging\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]  # \"v\" = verb\n",
    "\n",
    "print(\"Lemmatized Words (with POS):\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER identifies real-world entities in text, like names, places, dates, and organizations. 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Sandeep.C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Sandeep.C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Sandeep.C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')  #need to download when using nltk.ne_chunk() for NER for spaCy does't needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Elon/NNP)\n",
      "  (PERSON Musk/NNP)\n",
      "  founded/VBD\n",
      "  (ORGANIZATION SpaceX/NNP)\n",
      "  in/IN\n",
      "  (GPE California/NNP)\n",
      "  in/IN\n",
      "  2002/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"Elon Musk founded SpaceX in California in 2002.\"\n",
    "words = word_tokenize(text)\n",
    "pos_tags=pos_tag(words)\n",
    "ber_tree=ne_chunk(pos_tags)\n",
    "print(ber_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy provides faster and more accurate NER than nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Gates -> PERSON\n",
      "Microsoft -> ORG\n",
      "1975 -> DATE\n",
      "the United States -> GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Bill Gates founded Microsoft in 1975 in the United States.\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking & Chinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is like grouping words that belong together in a sentence. For example, in \"The quick brown fox\", all these words describe a noun (fox). Chunking helps us extract meaningful phrases like noun phrases (NP)\n",
    "Chinking is removing unwanted words from a chunk.\n",
    "For example, if we chunk \"The quick brown fox jumps,\" but we don’t want the verb \"jumps\", we chink (remove) it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Do We Need chunk_grammar and POS Tagging in Chunking?\n",
    "The chunk_grammar is a set of rules written using Regular Expressions (RegEx) to define which POS tags should be grouped into a chunk.\n",
    "Why Do We Use POS Tagging Before Chunking?\n",
    "POS tagging tells us the role of each word in a sentence. Without it, we wouldn't know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Step 1: Create a sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Step 2: Tokenize & Tag POS (Parts of Speech)\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Step 3: Define a Simple Chunk Rule\n",
    "chunk_grammar = r\"NP: {<DT>?<JJ>*<NN>}\"  # Noun Phrase: Optional Determiner, Adjectives, Noun\n",
    "\n",
    "chunk_parser = RegexpParser(chunk_grammar) # Step 4: Apply Chunking\n",
    "chunk_result = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(chunk_result) # Step 5: Print & Visualize Chunk Tree\n",
    "# chunk_result.draw()  # Opens a tree diagram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Parsing is a technique in NLP that helps understand the grammatical structure of a sentence by analyzing how words are related to each other.\n",
    "It identifies:\n",
    "Which word is the main verb (root)\n",
    "Which words depend on others (subjects, objects, modifiers, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The cat chased the mouse\n",
    "    chased (ROOT)\n",
    "     ├── cat (Subject)\n",
    "     ├── the (Determiner for \"cat\")\n",
    "     ├── mouse (Object)\n",
    "     ├── the (Determiner for \"mouse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --> det --> cat\n",
      "cat --> nsubj --> chased\n",
      "chased --> ROOT --> chased\n",
      "the --> det --> mouse\n",
      "mouse --> dobj --> chased\n",
      ". --> punct --> chased\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # Load English NLP model\n",
    "sentence = \"The cat chased the mouse.\" # Define sentence\n",
    "doc = nlp(sentence) # Process sentence\n",
    "for token in doc: # Print word dependencies\n",
    "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coreference Resolution\n",
    "\n",
    "Coreference resolution is the process of linking pronouns and noun phrases to the correct entities in a sentence or document.\n",
    "\n",
    "🔹 Example\n",
    "👉 \"John went to the market. He bought some apples.\"\n",
    "✅ Coreference Resolution: \"He\" → \"John\"\n",
    "\n",
    "👉 \"Sara met Priya. She gave her a book.\"\n",
    "✅ Coreference Resolution: \"She\" → \"Sara\", \"her\" → \"Priya\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Using neuralcoref for Coreference Resolution\n",
    "import spacy\n",
    "import neuralcoref  # Add coreference resolution to SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # Load SpaCy model\n",
    "neuralcoref.add_to_pipe(nlp) # Add NeuralCoref to SpaCy pipeline\n",
    "text = \"John went to the market. He bought some apples.\" # Example sentence\n",
    "doc = nlp(text) # Process the text\n",
    "print(doc._.coref_resolved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative \n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "# Load Coreference Model\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "text = \"Sara met Priya. She gave her a book.\"\n",
    "result = predictor.predict(document=text)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
